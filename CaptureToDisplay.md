# Capturing and Preprocessing the Image, and Displaying Results #

The FaceDetect midlet and its user interface are derived from the default midlet generated by the NetBeans development environment. It has not been extensively customized for this application. Menu commands and the FIRE button have been set in CameraCanvas to call the method takeSnapshot, which captures the image.

The video image is captured in takeSnapshot using the VideoControl method getSnapshot, passing null as the parameter. This returns an image using the default camera and parameters. Note that manipulating the camera and its format is somewhat difficult in J2ME because the cellphone manufacturer is allowed a great deal of freedom in implementing and supporting different image types and sizes. Using the default parameters at least guarantees the capture of an image. Haar-based face detection is sufficiently robust so that the image does not have to be especially high resolution to be captured.

Once the image is captured the DetectHaarParam Push method is called to convert from a javax.microedition.lcdui.Image to a jjil.core.RgbImage using RgbImage’s constructor. Next, the RgbImage is converted to a Gray8Image using RgbAvg2Gray, which averages the red, green, and blue pixels to produce a grayvalue. The result is then passed to DetectHaarMultiscale in jjil.algorithm using its Push method.

DetectHaarMultiscale is constructed in DetectHaarParam passing an input stream which contains a text representation of a Haar cascade (generated by Haar2J2ME, see below) and a minimum and maximum scale to its constructor. The scale parameters determine the minimum and maximum amount by which the image size is reduced before being passed to the Haar cascade. This is implemented in its Push method.

The output of DetectHaarMultiScale is a mask image indicating which areas of the image contain a face. The mask is initially zero. Starting at the coarsest scale, DetectHaarMultiScale’s Push method first reduces the image in size to that scale using GrayShrink, stretches the mask to the same size using GrayRectStretch, combines both into a Gray8MaskedImage using its constructor, and then passes the image to MaskGray8SubImgGen using its Push method to generate Gray8SubImage objects. Gray8SubImage takes one image, and generates a sequence of Gray8SubImage objects spaced across the image at fixed intervals. These are passed to DetectHaar using its Push method, which returns a boolean value indicating the presence or absence of a face. If a face is found, the corresponding region of the mask is filled in using Gray8Rect. Because we want to avoid rechecking an area of the image where a face has already been detected, the central pixel in the area being detected is checked before each subimage is passed to DetectHaar.

The process is the repeated at the next finer scale using the new mask. This method keeps the face detector from rechecking an area where a face has already been found while also providing the capability to detect multiple faces in the same image, as demonstrated previously.

After the process is complete, the mask is stretched to the same resolution as the input image, again using GrayRectStretch, and the result is made available to the Front method of DetectHaarMultiScale using the default implementation of setOutput.

DetectHaarParam shows the result by implementing Paint, which is called from CameraCanvas. When a result is available (i.e., DetectHaarMultiScale’s Empty member returns false) the mask and color input image are first combined using ApplyMaskRgb. This sets the background (non-masked region) to gray and also dims it, and leaves the foreground (masked region) in color. The result is then converted back to a javax.microedition.lcdui.Image using RgbImage’s getImage method and drawn to the display using drawImage.